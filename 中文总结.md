# OpenUnlearning 项目中文总结

## 项目概述

OpenUnlearning 是一个统一的大语言模型遗忘学习（LLM Unlearning）研究框架，支持多个基准测试（TOFU、MUSE、WMDP）、12+种遗忘方法、10+种评估指标和7+种模型架构。该框架使用 Hydra 进行配置管理，支持单GPU和分布式训练。

## 核心功能

### 支持的组件
- **基准测试**: TOFU（虚构遗忘）、MUSE（六维评估）、WMDP（危险知识遗忘）
- **遗忘方法**: GradAscent、GradDiff、NPO、SimNPO、DPO、RMU、UNDIAL、AltPO、SatImp、WGA、CE-U、PDU
- **评估指标**: 逐字概率、ROUGE分数、成员推理攻击、模型实用性、记忆化测试等
- **模型架构**: Llama-3.2、Llama-3.1、Llama-2、Phi-3.5、Gemma、Zephyr等

## 主要开发命令

### 环境设置
```bash
# 创建并激活conda环境
conda create -n unlearning python=3.11
conda activate unlearning

# 安装依赖包
pip install .[lm_eval]  # 支持lm-evaluation-harness
pip install .[dev]      # 开发工具（ruff, pre-commit）
pip install --no-build-isolation flash-attn==2.6.3

# 下载评估数据和日志
python setup_data.py --eval
```

### 代码质量检查
```bash
# 检查代码质量（linting和格式化）
make quality

# 应用格式化修复
make style

# 运行测试
make test
```

### 训练和评估
```bash
# 运行遗忘训练
python src/train.py --config-name=unlearn.yaml experiment=unlearn/tofu/default \
  forget_split=forget10 retain_split=retain90 trainer=GradAscent task_name=SAMPLE_UNLEARN

# 运行评估
python src/eval.py --config-name=eval.yaml experiment=eval/tofu/default \
  model=Llama-3.2-1B-Instruct task_name=SAMPLE_EVAL

# 分布式训练
CUDA_VISIBLE_DEVICES=0,1 accelerate launch \
  --config_file configs/accelerate/default_config.yaml --main_process_port 18765 \
  src/train.py --config-name=unlearn.yaml experiment=unlearn/muse/default task_name=DISTRIBUTED_TRAIN
```

## 架构设计

### 核心模块
- **src/train.py**: 主要训练入口，使用Hydra配置管理
- **src/eval.py**: 评估入口，用于运行基准测试
- **src/trainer/**: 包含遗忘方法实现
- **src/evals/**: 基准测试实现和评估指标
- **src/data/**: 数据预处理和数据集处理
- **src/model/**: 模型加载和配置工具

### 配置系统
使用Hydra进行分层配置管理：
- **configs/train.yaml**: 基础训练配置
- **configs/unlearn.yaml**: 遗忘学习特定配置
- **configs/eval.yaml**: 评估配置
- **configs/experiment/**: 常用实验预定义配置
- **configs/trainer/**: 方法特定的训练器配置
- **configs/model/**: 模型特定配置

### 输出目录结构
实验输出保存在 `./saves/${mode}/${task_name}`：
- `mode`: train/eval/unlearn
- `task_name`: 用户提供的实验标识符

## 关键特性

1. **Hydra集成**: 所有入口点都使用`@hydra.main()`装饰器进行配置管理
2. **组件工厂模式**: 通过工厂函数加载组件（`get_model()`、`get_data()`等）
3. **配置覆盖**: 命令行参数可以使用点记法覆盖任何配置参数
4. **分布式训练**: 使用Accelerate/DeepSpeed进行多GPU训练

## 开发指南

- 使用`ruff`进行代码质量控制
- 新组件应遵循工厂模式并在相应配置文件中注册
- 实验应使用描述性任务名称
- 提交前运行`make quality`检查代码质量
- 参考`docs/`目录中的详细组件实现指南

## 贡献方式

该项目欢迎社区贡献，包括：
- 添加新的遗忘方法
- 实现新的评估基准
- 支持更多模型架构
- 改进文档和教程
- 修复bugs和优化性能

项目采用MIT许可证，鼓励学术和工业界的广泛使用和改进。